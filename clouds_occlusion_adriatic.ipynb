{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_krmEyzemB9"
      },
      "source": [
        "# Sea Surface Temperature Reconstruction under cloud occlusion\n",
        "\n",
        "See Surface Temperature (SST) data are mostly acquired by means of satellites detecting the infrared radiation emitted from the sea surface. This radiation can be absorbed by clouds, causing large occlusions in collected observations. Filling these gaps is the task of your exam.\n",
        "\n",
        "We focus on a region of the north adriatic sea. Data are taken from the MODIS dataset; we use the nigthly data collected by the Aqua satellite.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "pM_hCSniwyC-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj8S4EYq7pSB"
      },
      "source": [
        "The next cells contain instructions for downloading data.\n",
        "**Please, make your own copy.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rjpg_lz9TT29",
        "outputId": "3b63305a-3a72-4350-86be-49ec89c71479"
      },
      "outputs": [],
      "source": [
        "#data\n",
        "!gdown 1cxZCt2CzMo9AolJ9k-je3b4w9t0Ibpvc\n",
        "!gdown 1L3PxazNUnc_cw9XvHfj_J-fhhBXv41YY\n",
        "!gdown 1tR7U8ndBktwiAhmpyQZG2nv5kRcX0mtd\n",
        "#dates\n",
        "!gdown 1ROGlbqknu47uyZs89J1oBml6vwE-wtfx\n",
        "!gdown 1wpKfWxwf9XXJixdUrrjN-idcAZ5H3-0C\n",
        "!gdown 1hGg-J7ipuil1Hp46YTIVWzXkSHc2avBE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYOOcuAPWefg",
        "outputId": "e4c39783-3c5e-41d1-e6bf-6e78c208d274"
      },
      "outputs": [],
      "source": [
        "#land-sea mask\n",
        "!gdown 1F0GGmrrGtSHizdf0COyHErNqMf9FazCK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB1yepYAYnGh",
        "outputId": "4ea8fa32-725b-4b5b-cfef-8c5671f21439"
      },
      "outputs": [],
      "source": [
        "#a statistical baseline\n",
        "!gdown 1JfVhw5HHlUOj_3UxVFT-kTaOVs6ZRUEJ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XUMXMVf8kdJ"
      },
      "source": [
        "Le us inspect and visualize data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4wFx-46Xt2s",
        "outputId": "41148cb1-5b00-42b3-f4f7-191e391a2c90"
      },
      "outputs": [],
      "source": [
        "x_train_n = np.load('./content/x_train_night.npy')\n",
        "x_val_n = np.load('./content/x_val_night.npy')\n",
        "x_test_n = np.load('./content/x_test_night.npy')\n",
        "\n",
        "print(x_train_n.shape)\n",
        "print(x_val_n.shape)\n",
        "print(x_test_n.shape)\n",
        "\n",
        "print(f\"min train temperature: {np.nanmin(x_train_n)}\")\n",
        "print(f\"max train temperature: {np.nanmax(x_train_n)}\")\n",
        "print(f\"mean train temperature: {np.nanmean(x_train_n)}\")\n",
        "print(f\"std train temperature: {np.nanstd(x_train_n)}\\n\")\n",
        "\n",
        "print(f\"min val temperature: {np.nanmin(x_val_n)}\")\n",
        "print(f\"max val temperature: {np.nanmax(x_val_n)}\\n\")\n",
        "\n",
        "print(f\"min test temperature: {np.nanmin(x_test_n)}\")\n",
        "print(f\"max test temperature: {np.nanmax(x_test_n)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llRRJ-m686Q6"
      },
      "source": [
        "Let us visualize a few images. Occluded area are coded with nans: they may correspond to land or clouds. nans are renderd in blank."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "T7ilwRTr9E9z",
        "outputId": "c9b39aaa-e8a8-4f86-8d42-bef414464f00"
      },
      "outputs": [],
      "source": [
        "i = np.random.randint(0, x_train_n.shape[0])\n",
        "plt.imshow(x_train_n[i]) #nans are blank; they could be land or clouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWIwCeIKaMP1",
        "outputId": "23cb195e-8439-4288-d942-8dcb3772e6f2"
      },
      "outputs": [],
      "source": [
        "dates_train_n = np.load('./content/dates_train_night.npy')\n",
        "dates_val_n = np.load('./content/dates_val_night.npy')\n",
        "dates_test_n = np.load('./content/dates_test_night.npy')\n",
        "\n",
        "print(dates_train_n.shape) # from 2002-07-04 to 2018-07-04\n",
        "print(dates_val_n.shape)   # from 2018-07-04 to 2021-07-04\n",
        "print(dates_test_n.shape)  # from 2021-07-04 to 2023-12-31"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGaPRwiJ9e2b"
      },
      "source": [
        "We know the part of the territory corresponding to land.\n",
        "In the land-sea mask, 1 is for sea, and 0 for land."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "qFjuEppqakIh",
        "outputId": "2d16403a-89f0-418f-8fa2-920cd1ae781a"
      },
      "outputs": [],
      "source": [
        "land_sea_mask = np.load('./content/land_sea_mask.npy')\n",
        "print(land_sea_mask.shape)\n",
        "plt.imshow(land_sea_mask)\n",
        "total_sea = np.sum(land_sea_mask)\n",
        "print(\"sea percentage = \", total_sea/(128*128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kveAvRNb-EH1"
      },
      "source": [
        "You can also make use of a statistical baseline, which already provides an interesting approximation. Your task is to make meaningful improvements to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_v7dluly8gY",
        "outputId": "820880d4-52de-479c-a22c-9c52252147b9"
      },
      "outputs": [],
      "source": [
        "baseline = np.load('./content/stat_baseline.npy')\n",
        "print(baseline.shape)\n",
        "print(f\"min baseline temperature: {np.nanmin(baseline)}\")\n",
        "print(f\"max baseline temperature: {np.nanmax(baseline)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "aiGhomd09txP",
        "outputId": "8030b2a0-1544-4464-c821-9b5869e77dda"
      },
      "outputs": [],
      "source": [
        "i = np.random.randint(0, baseline.shape[0])\n",
        "plt.imshow(baseline[i]) #nans are blank; they could be land or clouds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYzM_MvQ-PAz"
      },
      "source": [
        "# Normalization\n",
        "\n",
        "We perform gaussian normalization. You may use an alternative normalization if you prefer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "mt2CVq4YDEBN"
      },
      "outputs": [],
      "source": [
        "x_train_mean = np.nanmean(x_train_n)\n",
        "x_train_std = np.nanstd(x_train_n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "uJfj4aby-Go1"
      },
      "outputs": [],
      "source": [
        "x_train_n = (x_train_n - x_train_mean) / x_train_std\n",
        "x_val_n = (x_val_n - x_train_mean) / x_train_std\n",
        "x_test_n = (x_test_n - x_train_mean) / x_train_std\n",
        "\n",
        "baseline_nan = np.where(land_sea_mask,baseline,np.nan)\n",
        "baseline_nan = (baseline_nan - x_train_mean) / x_train_std\n",
        "baseline = np.where(land_sea_mask,baseline_nan,0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXWJ8XN-bTDQ"
      },
      "source": [
        "# Generator\n",
        "\n",
        "A problem with occluded data is that we do not have ground truth available, as we do not know the actual sea temperature beneath the clouds.\n",
        "\n",
        "To address this, we create an artificial ground truth by extending the clouded region. Specifically, we superimpose the clouds from a randomly chosen different day onto a given day, providing a configurable amount of auxiliary clouds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "PHpTyAvFzjRP"
      },
      "outputs": [],
      "source": [
        "def generator(batch_size, dataset, dates):\n",
        "    size = 128\n",
        "    while True:\n",
        "        batch_x = np.zeros((batch_size, size, size, 4))\n",
        "        batch_y = np.zeros((batch_size, size, size, 3))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            # Choose an image with a sufficiently large area of visible sea\n",
        "            found = False\n",
        "            while not found:\n",
        "              i = np.random.randint(0, dataset.shape[0])\n",
        "              visible = np.sum(~np.isnan(dataset[i])/total_sea)\n",
        "              if visible > 0.4:\n",
        "                found = True\n",
        "            image_current = np.nan_to_num(dataset[i], nan=0)\n",
        "            mask_current = np.isnan(dataset[i])\n",
        "\n",
        "            # Extending clouds\n",
        "\n",
        "            found = False\n",
        "            while not found:\n",
        "              r = np.random.randint(0, dataset.shape[0])\n",
        "              mask_r = np.isnan(dataset[r])\n",
        "              mask_or_r = np.logical_or(mask_current, mask_r)\n",
        "              nnans = np.sum(~mask_or_r)/total_sea\n",
        "              if nnans > 0.05 and nnans < min(visible-.1,0.4):\n",
        "                found = True\n",
        "\n",
        "            artificial_mask_current = ~mask_or_r  #1 visible, 0 masked\n",
        "\n",
        "            # Apply the enlarged mask to the current day's image\n",
        "            image_masked_current = np.where(artificial_mask_current, image_current, 0)\n",
        "\n",
        "            # we tune the statistical baseline according to the average sea temperature of the current day\n",
        "\n",
        "            # convert the current date to a datetime object using pandas\n",
        "            date_series = pd.to_datetime(dates[i], unit='D', origin='unix')\n",
        "            day_of_year = date_series.dayofyear\n",
        "\n",
        "            #avg temp of the current day\n",
        "            image_masked_nan = np.where(artificial_mask_current, image_current, np.nan)\n",
        "\n",
        "            avg_temp_real = np.nanmean(image_masked_nan)\n",
        "            avg_temp_baseline = np.nanmean(np.where(artificial_mask_current,baseline[day_of_year - 1],np.nan))\n",
        "            tuned_baseline = baseline[day_of_year - 1] + avg_temp_real - avg_temp_baseline  # Adjust the baseline to match the average temperature of the current day\n",
        "            tuned_baseline = np.where(land_sea_mask,tuned_baseline,.0)\n",
        "\n",
        "\n",
        "            # Removing nans for usage in neural networks\n",
        "\n",
        "            mask_current = np.logical_not(mask_current) # 1 for clear sea, 0 for land/clouds\n",
        "            diff_mask = np.logical_and(~artificial_mask_current,mask_current) # 1 for clear sea, 0 for land/clouds and artificial clo\n",
        "\n",
        "            # Create batch_x and batch_y\n",
        "            batch_x[b, ..., 0] = image_masked_current            #current artificially clouded image\n",
        "            batch_x[b, ..., 1] = artificial_mask_current         #current artificial mask\n",
        "            batch_x[b, ..., 2] = land_sea_mask                   #land-sea mask\n",
        "            batch_x[b, ..., 3] = tuned_baseline                  #tuned baseline\n",
        "\n",
        "            batch_y[b, ..., 0] = image_current                   #real image\n",
        "            batch_y[b, ..., 1] = mask_current                    #real mask\n",
        "            batch_y[b, ..., 2] = diff_mask                       #artificial mask used for the input\n",
        "\n",
        "        yield batch_x, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rGi0Cl5t-UOs"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_gen = generator(batch_size, x_train_n, dates_train_n)\n",
        "val_gen = generator(batch_size, x_val_n, dates_val_n)\n",
        "test_gen = generator(batch_size, x_test_n, dates_test_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH0yk3zswxd6"
      },
      "source": [
        "The generator returns two sets of data, called batch_x and batch_y. The idea is that batch_x data are possible inputs for the neural network, while batch_y data provide ground_truth information that can be used for defining the loss function, or auxiliary metrics. Let us inspect them in more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "wY4dMhgq5Grn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_elements(images, titles):\n",
        "\n",
        "  num_of_images = len(images)\n",
        "  rows = 1\n",
        "  cols = num_of_images\n",
        "  mini = np.nanmin(np.array(images))\n",
        "  maxi = np.nanmax(np.array(images))\n",
        "  print(mini,maxi)\n",
        "\n",
        "  plt.figure(figsize=(10, 10*num_of_images))\n",
        "  for i in range(num_of_images):\n",
        "    print(f\"image {i}; {np.min(images[i])}, {np.max(images[i])}\")\n",
        "    plt.subplot(rows, cols, i+1)\n",
        "    plt.imshow(images[i],vmin=mini,vmax=maxi)\n",
        "    plt.axis('off')\n",
        "    plt.title(titles[i])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "1-vtQxrX45j2",
        "outputId": "9e24cbc5-42e5-4f88-c667-f66318d8377f"
      },
      "outputs": [],
      "source": [
        "batch_x, batch_y = next(test_gen)\n",
        "assert(np.sum(np.isnan(batch_x))==0)\n",
        "assert(np.sum(np.isnan(batch_y))==0)\n",
        "real_denorm = np.where(batch_y[0,...,1],batch_y[0,...,0]*x_train_std + x_train_mean,0.0)\n",
        "artificial_denorm = np.where(batch_x[0,...,1],batch_y[0,...,0]*x_train_std + x_train_mean,0.0)\n",
        "baseline_denorm = np.where(land_sea_mask,batch_x[0,...,3]*x_train_std + x_train_mean,0.0)\n",
        "\n",
        "plot_elements([real_denorm, artificial_denorm, baseline_denorm], ['real', 'input','baseline'])\n",
        "plot_elements([batch_y[0,...,1], batch_x[0,...,1], batch_y[0,...,2]],[\"real mask\",\"artificial mask\",\"diff mask\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi9F4orQOFvQ"
      },
      "source": [
        "In the first row, we see:\n",
        "- the real image of the day\n",
        "- the input passed to the model, that is a masked version of the previous image\n",
        "- the tentative reconstruction done via the statistical baseline\n",
        "All images have been denormalized.\n",
        "\n",
        "In the second row we see:\n",
        "- the mask of the current day\n",
        "- the enlarged masked with addiitonal clouds\n",
        "- the difference, showing the portion of the image visible in the first one, but occluded in the second. This is the region where the performance of the model must be evaluated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8UxBp3MaU4-"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "The model must be evaluted by means of the following function, calling the test generator 50 times. The metrics adopted is the Rooted Mean Squared Error (RMSE).\n",
        "\n",
        "To make a running example, we measure the performance of the statistical\n",
        "baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg2XAb5xyWOT",
        "outputId": "a4081fdc-3d5e-43e6-9731-b12c403efd59"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store the errors and the maximum errors\n",
        "RMSE = []\n",
        "\n",
        "# Generate and evaluate tot batches\n",
        "tot = 50\n",
        "for _ in range(tot):\n",
        "    # Generate a batch\n",
        "    batch_x, batch_y = next(test_gen)\n",
        "    #uncomment the next line and call your model\n",
        "    #predictions = model.predict(x_true, verbose=0)\n",
        "    predictions = batch_x[...,3] #use the baseline as prediction\n",
        "\n",
        "    # Denormalize data !!!\n",
        "    predictions_denorm = predictions*x_train_std + x_train_mean\n",
        "    true_values_denorm = batch_y[..., 0]*x_train_std + x_train_mean\n",
        "\n",
        "    # Get the masks and calculate the errors\n",
        "    diffMask = batch_y[..., 2]\n",
        "    diff_errors_batch = np.where(diffMask, np.abs(predictions_denorm - true_values_denorm), np.nan)\n",
        "    squared_errors = np.nanmean(diff_errors_batch**2,axis=(1,2))\n",
        "    RMSE.append(np.sqrt(squared_errors))\n",
        "\n",
        "RMSE = np.concatenate(RMSE)\n",
        "\n",
        "print(f\"RMSE :\", np.mean(RMSE))\n",
        "print(f\"RMSE std :\", np.std(RMSE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5kdD0nLh3P7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBJ9phY-cXG7"
      },
      "source": [
        "# Remarks\n",
        "\n",
        "All data in batch_x can be used as input to the model. You may choose to use only a subset of the data or perform additional preprocessing if desired.\n",
        "\n",
        "Do not modify the test generator, as this could affect the evaluation of the model.\n",
        "\n",
        "The notebook should include the code for a single model. You may discuss additional experiments by briefly mentioning the results.\n",
        "\n",
        "The project must be written in tensorflow, as usual.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def masked_loss(y_true, y_pred):\n",
        "\n",
        "    mask = y_true[..., 1]\n",
        "    \n",
        "    true_values = y_true[..., 0]\n",
        "    \n",
        "    diff = mask * (true_values - y_pred[..., 0])\n",
        "    \n",
        "    mse = K.sum(K.square(diff)) / K.sum(mask)\n",
        "    \n",
        "    return mse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.models as models\n",
        "\n",
        "\n",
        "def unet_model(input_size=(128, 128, 4)):\n",
        "    inputs = layers.Input(input_size)\n",
        "\n",
        "    # Encoder\n",
        "    c1 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
        "\n",
        "    # Bottleneck\n",
        "    b = layers.Conv2D(2048, (3, 3), activation='relu', padding='same')(p4)\n",
        "    b = layers.Conv2D(2048, (3, 3), activation='relu', padding='same')(b)\n",
        "\n",
        "    # Decoder\n",
        "    u1 = layers.Conv2DTranspose(1024, (2, 2), strides=(2, 2), padding='same')(b)\n",
        "    u1 = layers.concatenate([u1, c4])\n",
        "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(u1)\n",
        "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
        "\n",
        "    u2 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u2 = layers.concatenate([u2, c3])\n",
        "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u2)\n",
        "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
        "\n",
        "    u3 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u3 = layers.concatenate([u3, c2])\n",
        "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u3)\n",
        "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
        "\n",
        "    u4 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u4 = layers.concatenate([u4, c1])\n",
        "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u4)\n",
        "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
        "\n",
        "    # Output layer - one output channel for the reconstructed SST values\n",
        "    outputs = layers.Conv2D(3, (1, 1), activation='linear')(c8)\n",
        "\n",
        "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "model = unet_model()\n",
        "optimizer = Adam(learning_rate=1e-6)\n",
        "model.compile(optimizer=optimizer, loss=masked_loss)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "model.fit(train_gen, validation_data=val_gen, callbacks=[early_stopping], epochs=150, steps_per_epoch=100, validation_steps=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize lists to store the errors and the maximum errors\n",
        "RMSE = []\n",
        "\n",
        "# Generate and evaluate tot batches\n",
        "tot = 50\n",
        "for _ in range(tot):\n",
        "    # Generate a batch\n",
        "    batch_x, batch_y = next(test_gen)\n",
        "    #uncomment the next line and call your model\n",
        "    predictions = model.predict(batch_x, verbose=0)\n",
        "    predictions = predictions[...,0]\n",
        "    #predictions = batch_x[...,3] #use the baseline as prediction\n",
        "\n",
        "    # Denormalize data !!!\n",
        "    predictions_denorm = predictions*x_train_std + x_train_mean\n",
        "    true_values_denorm = batch_y[..., 0]*x_train_std + x_train_mean\n",
        "\n",
        "    # Get the masks and calculate the errors\n",
        "    diffMask = batch_y[..., 2]\n",
        "    diff_errors_batch = np.where(diffMask, np.abs(predictions_denorm - true_values_denorm), np.nan)\n",
        "    squared_errors = np.nanmean(diff_errors_batch**2,axis=(1,2))\n",
        "    RMSE.append(np.sqrt(squared_errors))\n",
        "\n",
        "RMSE = np.concatenate(RMSE)\n",
        "\n",
        "print(f\"RMSE :\", np.mean(RMSE))\n",
        "print(f\"RMSE std :\", np.std(RMSE))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
